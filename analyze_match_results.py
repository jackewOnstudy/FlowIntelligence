#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€åŒ¹é…ç»“æœå‡†ç¡®ç‡ç»Ÿè®¡è„šæœ¬
åˆ†æ MultiModal_Output ç›®å½•ä¸‹çš„åŒ¹é…ç»“æœ

ä½¿ç”¨ç¤ºä¾‹:
1. åˆ†ææ‰€æœ‰æ¨¡æ€:
   python analyze_match_results.py

2. æ’é™¤OpticalFlowæ¨¡æ€:
   python analyze_match_results.py --exclude_modalities OpticalFlow

3. æ’é™¤å¤šä¸ªæ¨¡æ€:
   python analyze_match_results.py --exclude_modalities OpticalFlow FreqDomain

4. æŒ‡å®šè‡ªå®šä¹‰è¾“å‡ºç›®å½•å’ŒExcelæ–‡ä»¶å:
   python analyze_match_results.py --output_dir /path/to/output --excel_output custom_results.xlsx

5. æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯:
   python analyze_match_results.py --help
"""

import os
import glob
import pandas as pd
import numpy as np
from collections import defaultdict
from typing import Dict, List, Tuple
import json
import cv2


def get_video_resolution_from_match_path(match_path: str, video_root: str = "/media/jackew/Extreme SSD/newData/resized") -> tuple:
    """
    ä»åŒ¹é…ç»“æœè·¯å¾„ä¸­æå–åœºæ™¯åï¼Œè¯»å–å¯¹åº”è§†é¢‘çš„åˆ†è¾¨ç‡ã€‚
    
    Args:
        match_path: åŒ¹é…ç»“æœæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ MultiModal_Output_ransac_5_05/K1/...
        video_root: è§†é¢‘æ ¹ç›®å½•ï¼Œé»˜è®¤ /media/jackew/Extreme SSD/oldData/renamed
    
    Returns:
        (width, height) åˆ†è¾¨ç‡å…ƒç»„ï¼›å¦‚æœæ‰¾ä¸åˆ°è§†é¢‘æˆ–è¯»å–å¤±è´¥ï¼Œè¿”å› None
    """
    # 1. æå–åœºæ™¯å
    parts = os.path.normpath(match_path).split(os.sep)
    try:
        k_index = parts.index('MultiModal_Output') + 1
        scene_name = parts[k_index]  # æ¯”å¦‚ "K1"
    except (ValueError, IndexError):
        print(f"âŒ æ— æ³•ä»è·¯å¾„ä¸­æå–åœºæ™¯å: {match_path}")
        return None

    # 2. æ„é€ è§†é¢‘è·¯å¾„
    video_file = os.path.join(video_root, f"{scene_name}.mp4")
    if not os.path.exists(video_file):
        print(f"âŒ è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {video_file}")
        return None

    # 3. ä½¿ç”¨ OpenCV è·å–è§†é¢‘åˆ†è¾¨ç‡
    cap = cv2.VideoCapture(video_file)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_file}")
        return None

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()

    print(f"ğŸ¥ è§†é¢‘ {scene_name}.mp4 åˆ†è¾¨ç‡: {width} x {height}")
    return (width, height)


def patch_id_to_coords(patch_id: int, patch_size: int, image_width: int) -> Tuple[float, float]:
    """
    å°† patch ID è½¬æ¢ä¸ºå›¾åƒåæ ‡ï¼ˆä¸­å¿ƒç‚¹ï¼‰
    - patch_size: 8, 16, 32, ...
    - image_width: å›¾åƒå®½åº¦ï¼ˆåƒç´ ï¼‰
    """
    cols_per_row = image_width // patch_size
    row = patch_id // cols_per_row
    col = patch_id % cols_per_row
    x = col * patch_size + patch_size / 2
    y = row * patch_size + patch_size / 2
    return (x, y)

def load_match_ids_from_txt(file_path: str) -> List[Tuple[int, int]]:
    """
    è¯»å–åŒ¹é…txtæ–‡ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    - ç¬¬ä¸€è¡Œæ˜¯åŒ¹é…æ•°é‡ï¼ˆæ•´æ•°ï¼‰
    - åç»­æ¯è¡Œæœ‰3ä¸ªæ•°å­—ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œæ ¼å¼å¦‚ id1 id2 value
    è¿”å›åªæå– (id1, id2) çš„åˆ—è¡¨
    """
    matches = []
    print(f"ğŸ“„ è¯»å–åŒ¹é…æ–‡ä»¶: {file_path}")
    with open(file_path, 'r') as f:
        lines = f.readlines()
        if not lines:
            return matches
        
        try:
            total_matches = int(lines[0].strip())  # è¯»å–ç¬¬ä¸€è¡ŒåŒ¹é…æ•°é‡
        except:
            total_matches = None  # å¦‚æœç¬¬ä¸€è¡Œä¸æ˜¯æ•´æ•°ä¹Ÿä¸æŠ¥é”™ï¼Œå¿½ç•¥
        
        for line in lines[1:]:  # è·³è¿‡ç¬¬ä¸€è¡Œ
            try:
                parts = line.strip().split()  # é»˜è®¤æŒ‰ç©ºç™½åˆ†å‰²
                if len(parts) < 2:
                    continue
                id1, id2 = int(parts[0]), int(parts[1])
                matches.append((id1, id2))
            except:
                continue
    
    return matches


def calculate_accuracy(match_file_path: str) -> Dict[int, float]:

    use_pt_level = True
    """
    ä½¿ç”¨å•ä½çŸ©é˜µè¯„ä¼°åŒ¹é…å‡†ç¡®ç‡ï¼Œè¯¯å·®é˜ˆå€¼ä» 1 åˆ° 10 åƒç´ ã€‚
    ä¸ä½¿ç”¨ RANSACï¼Œä»…è¯„ä¼°å˜æ¢è¯¯å·®ã€‚

    Returns:
        Dict[int, float]: key æ˜¯åƒç´ è¯¯å·®é˜ˆå€¼ï¼ˆ1-10ï¼‰ï¼Œvalue æ˜¯è¯¥é˜ˆå€¼ä¸‹çš„å‡†ç¡®ç‡
    """
    try:
        matches = load_match_ids_from_txt(match_file_path)
        if not matches or len(matches) < 1:
            print(f"âŒ åŒ¹é…æ•°æ®ä¸ºç©ºæˆ–æ— æ•ˆ: {matches}")
            return {k: 0.0 for k in range(1, 11)}

        filename = os.path.basename(match_file_path)
        if 'x' not in filename:
            print(f"âŒ æ–‡ä»¶åæ— æ³•è¯†åˆ« patch size: {filename}")
            return {k: 0.0 for k in range(1, 11)}

        patch_size = int(filename.split('x')[0])
        res = get_video_resolution_from_match_path(match_file_path)
        if res is None:
            return {k: 0.0 for k in range(1, 11)}
        width, height = res

        # è®¡ç®—çœŸå®åæ ‡
        src_pts = np.array([patch_id_to_coords(m[0], patch_size, width) for m in matches])
        dst_pts = np.array([patch_id_to_coords(m[1], patch_size, width) for m in matches])

        ransac_threshold = max(3.0, patch_size * 0.5)  # åŠ¨æ€é˜ˆå€¼
        max_iters = 2000  # æœ€å¤§è¿­ä»£æ¬¡æ•°
        confidence = 0.95  # ç½®ä¿¡åº¦

    # è®¡ç®—å•åº”æ€§çŸ©é˜µ
        H, mask = cv2.findHomography(
            src_pts, dst_pts,
            method=cv2.RANSAC,
            ransacReprojThreshold=ransac_threshold,
            maxIters=max_iters,
            confidence=confidence
        )

        if mask is None:
            print(f"âš ï¸ RANSAC è¿”å›æ— æ•ˆ mask: {match_file_path}")
            return {k: 0.0 for k in range(1, 11)}

        # æå–å†…ç‚¹
        inlier_src = src_pts[mask.ravel() == 1]
        inlier_dst = dst_pts[mask.ravel() == 1]
        

        # # ä½¿ç”¨å•ä½çŸ©é˜µï¼Œå³ä¸è¿›è¡Œä»»ä½•å˜æ¢
        # transformed_pts = src_pts  # å› ä¸º H = I

        # è®¡ç®—æ¬§å‡ é‡Œå¾—è¯¯å·®
        if not use_pt_level:
            errors = np.linalg.norm(inlier_src - inlier_dst, axis=1)
            total = len(errors)
            acc_by_thresh = {}
            for thresh in range(1, 11):
                inliers = np.sum(errors <= thresh)
                acc_by_thresh[thresh] = inliers / total if total > 0 else 0.0

            return acc_by_thresh
        else:
            dx = np.abs(inlier_src[:, 0] - inlier_dst[:, 0])
            dy = np.abs(inlier_src[:, 1] - inlier_dst[:, 1])

            errors = np.maximum(dx, dy)
            total = len(errors)
            acc_by_thresh = {}
            for thresh in [4,8,12,16,20]:
                inliers = np.sum(errors <= thresh)
                acc_by_thresh[thresh] = inliers / total if total > 0 else 0.0

            return acc_by_thresh

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {match_file_path} - {str(e)}")
        return {k: 0.0 for k in range(1, 11)}




# def calculate_accuracy(match_file_path: str) -> float:
#     """
#     è®¡ç®—åŒ¹é…ç»“æœæ–‡ä»¶çš„å‡†ç¡®ç‡
#     è¿™ä¸ªå‡½æ•°éœ€è¦æ ¹æ®å®é™…çš„å‡†ç¡®ç‡è®¡ç®—æ–¹æ³•æ¥å®ç°
#     ç°åœ¨ä½œä¸ºå ä½ç¬¦ï¼Œè¿”å›ä¸€ä¸ªç¤ºä¾‹å€¼
    
#     Args:
#         match_file_path: åŒ¹é…ç»“æœæ–‡ä»¶è·¯å¾„
        
#     Returns:
#         å‡†ç¡®ç‡å€¼ (0-1ä¹‹é—´çš„æµ®ç‚¹æ•°)
#     """
#     # TODO: å®ç°å…·ä½“çš„å‡†ç¡®ç‡è®¡ç®—é€»è¾‘
#     # è¿™é‡Œåªæ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œéœ€è¦æ ¹æ®å®é™…çš„åŒ¹é…ç»“æœæ–‡ä»¶æ ¼å¼æ¥å®ç°
#     try:
#         with open(match_file_path, 'r') as f:
#             lines = f.readlines()
#             # è¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„å‡†ç¡®ç‡è®¡ç®—é€»è¾‘
#             # ç°åœ¨è¿”å›ä¸€ä¸ªåŸºäºæ–‡ä»¶è¡Œæ•°çš„ç¤ºä¾‹å€¼
#             return min(1.0, len(lines) / 100.0)
#     except Exception as e:
#         print(f"Error reading {match_file_path}: {e}")
#         return 0.0

class MatchResultAnalyzer:
    def __init__(self, output_dir: str, exclude_modalities: List[str] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            output_dir: MultiModal_Output_ransac_5_05 ç›®å½•è·¯å¾„
            exclude_modalities: è¦æ’é™¤çš„æ¨¡æ€åˆ—è¡¨ï¼Œä¾‹å¦‚ ['OpticalFlow', 'FreqDomain']
        """
        self.output_dir = output_dir
        self.exclude_modalities = exclude_modalities or []
        self.scenarios = []
        self.modality_pairs = []
        self.grid_sizes = ['8x8', '16x16', '32x32', '64x64']
        
        # å­˜å‚¨æ‰€æœ‰ç»Ÿè®¡ç»“æœ
        self.scenario_modality_results = defaultdict(lambda: defaultdict(dict))  # scenario -> modality_pair -> grid_size -> accuracy
        self.scenario_averages = defaultdict(dict)  # scenario -> grid_size -> average_accuracy
        self.modality_averages = defaultdict(dict)  # modality_pair -> grid_size -> average_accuracy
        self.global_averages = {}  # grid_size -> average_accuracy
        
    def _should_exclude_modality_pair(self, modality_pair: str) -> bool:
        """
        æ£€æŸ¥æ¨¡æ€é…å¯¹æ˜¯å¦åº”è¯¥è¢«æ’é™¤
        
        Args:
            modality_pair: æ¨¡æ€é…å¯¹åç§°ï¼Œå¦‚ 'FreqDomain_vs_OpticalFlow'
            
        Returns:
            True å¦‚æœåº”è¯¥æ’é™¤ï¼ŒFalse å¦‚æœåº”è¯¥ä¿ç•™
        """
        if not self.exclude_modalities:
            return False
        
        # æ£€æŸ¥æ¨¡æ€é…å¯¹ä¸­æ˜¯å¦åŒ…å«ä»»ä½•éœ€è¦æ’é™¤çš„æ¨¡æ€
        for exclude_modality in self.exclude_modalities:
            if exclude_modality in modality_pair:
                return True
        
        return False

    def scan_directory_structure(self):
        """æ‰«æç›®å½•ç»“æ„ï¼Œè·å–æ‰€æœ‰åœºæ™¯å’Œæ¨¡æ€é…å¯¹"""
        self.scenarios = []
        self.modality_pairs = set()
        
        for item in os.listdir(self.output_dir):
            if os.path.isdir(os.path.join(self.output_dir, item)) and item.startswith('V'):
                self.scenarios.append(item)
                
                # æ‰«æè¯¥åœºæ™¯ä¸‹çš„æ¨¡æ€é…å¯¹
                scenario_dir = os.path.join(self.output_dir, item)
                for modality_item in os.listdir(scenario_dir):
                    modality_path = os.path.join(scenario_dir, modality_item)
                    if os.path.isdir(modality_path) and '_vs_' in modality_item:
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤è¯¥æ¨¡æ€é…å¯¹
                        if not self._should_exclude_modality_pair(modality_item):
                            self.modality_pairs.add(modality_item)
        
        self.scenarios.sort(key=lambda x: int(x[1:]))  # æŒ‰æ•°å­—æ’åº
        self.modality_pairs = sorted(list(self.modality_pairs))
        
        print(f"å‘ç° {len(self.scenarios)} ä¸ªåœºæ™¯: {self.scenarios}")
        print(f"å‘ç° {len(self.modality_pairs)} ä¸ªæ¨¡æ€é…å¯¹: {self.modality_pairs}")
        
        if self.exclude_modalities:
            print(f"å·²æ’é™¤åŒ…å«ä»¥ä¸‹æ¨¡æ€çš„é…å¯¹: {self.exclude_modalities}")
    
    def find_match_files(self, scenario: str, modality_pair: str) -> Dict[str, str]:
        """
        æ‰¾åˆ°æŒ‡å®šåœºæ™¯å’Œæ¨¡æ€é…å¯¹çš„åŒ¹é…ç»“æœæ–‡ä»¶
        
        Args:
            scenario: åœºæ™¯åç§°ï¼Œå¦‚ 'K1'
            modality_pair: æ¨¡æ€é…å¯¹åç§°ï¼Œå¦‚ 'FreqDomain_vs_MotionThermal'
            
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºç½‘æ ¼å¤§å°ï¼Œå€¼ä¸ºæ–‡ä»¶è·¯å¾„
        """
        match_files = {}
        
        # æ„å»ºè·¯å¾„ï¼šscenario/modality_pair/MatchResult/List/å­æ–‡ä»¶å¤¹/
        base_path = os.path.join(self.output_dir, scenario, modality_pair, 'MatchResult', 'List')
        
        if not os.path.exists(base_path):
            return match_files
        
        # æ‰¾åˆ°Listä¸‹çš„å­æ–‡ä»¶å¤¹ï¼ˆæ— æ„ä¹‰çš„å±‚çº§æ–‡ä»¶å¤¹ï¼‰
        try:
            subdirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
            if not subdirs:
                return match_files
            
            # å–ç¬¬ä¸€ä¸ªå­æ–‡ä»¶å¤¹
            subdir = subdirs[0]
            match_dir = os.path.join(base_path, subdir)
            
            # å¯»æ‰¾ç½‘æ ¼å¤§å°æ–‡ä»¶
            for grid_size in self.grid_sizes:
                file_path = os.path.join(match_dir, f"{grid_size}.txt")
                if os.path.exists(file_path):
                    match_files[grid_size] = file_path
                    
        except Exception as e:
            print(f"Error scanning {base_path}: {e}")
        
        return match_files
    
    def analyze_scenario_modality(self, scenario: str, modality_pair: str):
        """åˆ†æç‰¹å®šåœºæ™¯å’Œæ¨¡æ€é…å¯¹çš„ç»“æœ"""
        match_files = self.find_match_files(scenario, modality_pair)
        
        for grid_size, file_path in match_files.items():
            acc_dict = calculate_accuracy(file_path)
            self.scenario_modality_results[scenario][modality_pair][grid_size] = acc_dict
            print(f"  {scenario} - {modality_pair} - {grid_size}: {acc_dict}")
    
    def calculate_scenario_averages(self):
        """è®¡ç®—æ¯ä¸ªåœºæ™¯æ‰€æœ‰æ¨¡æ€åŒ¹é…ç»“æœçš„ä¸åŒç½‘æ ¼å¤§å°å¹³å‡å‡†ç¡®ç‡"""
        for scenario in self.scenarios:
            for grid_size in self.grid_sizes:
                acc_by_thresh = defaultdict(list)  # æ¯ä¸ªåƒç´ é˜ˆå€¼ -> æ‰€æœ‰æ¨¡æ€å‡†ç¡®ç‡åˆ—è¡¨

                for modality_pair in self.modality_pairs:
                    acc_dict = self.scenario_modality_results.get(scenario, {}).get(modality_pair, {}).get(grid_size)
                    if acc_dict:
                        for thresh, acc in acc_dict.items():
                            acc_by_thresh[thresh].append(acc)

                # è®¡ç®—æ¯ä¸ªåƒç´ é˜ˆå€¼çš„å¹³å‡å‡†ç¡®ç‡
                if acc_by_thresh:
                    self.scenario_averages[scenario][grid_size] = {
                        thresh: sum(accs) / len(accs) for thresh, accs in acc_by_thresh.items()
                    }

    
    def calculate_modality_averages(self):
        """è®¡ç®—æ¯ä¸ªæ¨¡æ€åœ¨æ‰€æœ‰åœºæ™¯ä¸‹çš„ä¸åŒç½‘æ ¼å¤§å°å¹³å‡å‡†ç¡®ç‡"""
        for modality_pair in self.modality_pairs:
            for grid_size in self.grid_sizes:
                acc_by_thresh = defaultdict(list)

                for scenario in self.scenarios:
                    acc_dict = self.scenario_modality_results.get(scenario, {}).get(modality_pair, {}).get(grid_size)
                    if acc_dict:
                        for thresh, acc in acc_dict.items():
                            acc_by_thresh[thresh].append(acc)

                if acc_by_thresh:
                    self.modality_averages[modality_pair][grid_size] = {
                        thresh: sum(accs) / len(accs) for thresh, accs in acc_by_thresh.items()
                    }

    
    def calculate_global_averages(self):
        """è®¡ç®—æ‰€æœ‰åœºæ™¯çš„ä¸åŒç½‘æ ¼å¹³å‡å‡†ç¡®ç‡ï¼ˆæ¯ä¸ªåƒç´ é˜ˆå€¼ï¼‰"""
        for grid_size in self.grid_sizes:
            acc_by_thresh = defaultdict(list)

            for scenario in self.scenarios:
                for modality_pair in self.modality_pairs:
                    acc_dict = self.scenario_modality_results.get(scenario, {}).get(modality_pair, {}).get(grid_size)
                    if acc_dict:
                        for thresh, acc in acc_dict.items():
                            acc_by_thresh[thresh].append(acc)

            if acc_by_thresh:
                self.global_averages[grid_size] = {
                    thresh: sum(accs) / len(accs) for thresh, accs in acc_by_thresh.items()
                }

    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        print("å¼€å§‹åˆ†æåŒ¹é…ç»“æœ...")
        
        # 1. æ‰«æç›®å½•ç»“æ„
        self.scan_directory_structure()
        
        # 2. åˆ†ææ¯ä¸ªåœºæ™¯çš„æ¯ä¸ªæ¨¡æ€é…å¯¹
        print("\næ­£åœ¨åˆ†æå„åœºæ™¯å’Œæ¨¡æ€é…å¯¹çš„å‡†ç¡®ç‡...")
        for scenario in self.scenarios:
            print(f"åˆ†æåœºæ™¯ {scenario}:")
            for modality_pair in self.modality_pairs:
                self.analyze_scenario_modality(scenario, modality_pair)
        
        # 3. è®¡ç®—å„ç§å¹³å‡å€¼
        print("\nè®¡ç®—åœºæ™¯å¹³å‡å‡†ç¡®ç‡...")
        self.calculate_scenario_averages()
        
        print("è®¡ç®—æ¨¡æ€å¹³å‡å‡†ç¡®ç‡...")
        self.calculate_modality_averages()
        
        print("è®¡ç®—å…¨å±€å¹³å‡å‡†ç¡®ç‡...")
        self.calculate_global_averages()
        
        print("\nåˆ†æå®Œæˆï¼")
    
    def save_results_to_excel(self, output_file: str = "match_results_analysis.xlsx"):
        """å°†ç»“æœä¿å­˜åˆ°Excelæ–‡ä»¶"""
        # å¦‚æœæœ‰æ’é™¤æ¨¡æ€ï¼Œåœ¨æ–‡ä»¶åä¸­ä½“ç°
        if self.exclude_modalities:
            base_name, ext = os.path.splitext(output_file)
            excluded_str = "_excluded_" + "_".join(self.exclude_modalities)
            output_file = f"{base_name}{excluded_str}{ext}"
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # 1. æ¯ä¸ªåœºæ™¯æ¯ä¸ªæ¨¡æ€çš„è¯¦ç»†ç»“æœ
            detailed_data = []
            for scenario in self.scenarios:
                for modality_pair in self.modality_pairs:
                    if scenario in self.scenario_modality_results and modality_pair in self.scenario_modality_results[scenario]:
                        row_data = {'åœºæ™¯': scenario, 'æ¨¡æ€é…å¯¹': modality_pair}
                        for grid_size in self.grid_sizes:
                            if grid_size in self.scenario_modality_results[scenario][modality_pair]:
                                row_data[f'å‡†ç¡®ç‡_{grid_size}'] = self.scenario_modality_results[scenario][modality_pair][grid_size]
                            else:
                                row_data[f'å‡†ç¡®ç‡_{grid_size}'] = None
                        detailed_data.append(row_data)
            
            detailed_df = pd.DataFrame(detailed_data)
            detailed_df.to_excel(writer, sheet_name='è¯¦ç»†ç»“æœ', index=False)
            
            # 2. åœºæ™¯å¹³å‡å‡†ç¡®ç‡
            scenario_avg_data = []
            for scenario in self.scenarios:
                if scenario in self.scenario_averages:
                    row_data = {'åœºæ™¯': scenario}
                    for grid_size in self.grid_sizes:
                        if grid_size in self.scenario_averages[scenario]:
                            row_data[f'å¹³å‡å‡†ç¡®ç‡_{grid_size}'] = self.scenario_averages[scenario][grid_size]
                        else:
                            row_data[f'å¹³å‡å‡†ç¡®ç‡_{grid_size}'] = "NA"
                    scenario_avg_data.append(row_data)
            
            scenario_avg_df = pd.DataFrame(scenario_avg_data)
            scenario_avg_df.to_excel(writer, sheet_name='åœºæ™¯å¹³å‡å‡†ç¡®ç‡', index=False)
            
            # 3. æ¨¡æ€å¹³å‡å‡†ç¡®ç‡
            modality_avg_data = []
            for modality_pair in self.modality_pairs:
                if modality_pair in self.modality_averages:
                    row_data = {'æ¨¡æ€é…å¯¹': modality_pair}
                    for grid_size in self.grid_sizes:
                        if grid_size in self.modality_averages[modality_pair]:
                            row_data[f'å¹³å‡å‡†ç¡®ç‡_{grid_size}'] = self.modality_averages[modality_pair][grid_size]
                        else:
                            row_data[f'å¹³å‡å‡†ç¡®ç‡_{grid_size}'] = None
                    modality_avg_data.append(row_data)
            
            modality_avg_df = pd.DataFrame(modality_avg_data)
            modality_avg_df.to_excel(writer, sheet_name='æ¨¡æ€å¹³å‡å‡†ç¡®ç‡', index=False)
            
            # 4. å…¨å±€å¹³å‡å‡†ç¡®ç‡
            global_avg_data = [{'ç½‘æ ¼å¤§å°': grid_size, 'å…¨å±€å¹³å‡å‡†ç¡®ç‡': self.global_averages.get(grid_size, None)} 
                             for grid_size in self.grid_sizes]
            global_avg_df = pd.DataFrame(global_avg_data)
            global_avg_df.to_excel(writer, sheet_name='å…¨å±€å¹³å‡å‡†ç¡®ç‡', index=False)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ° {output_file}")
    
    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*80)
        print("åŒ¹é…ç»“æœç»Ÿè®¡æ‘˜è¦")
        print("="*80)

        grid_size_key = '8x8'

        # å…¨å±€å¹³å‡å‡†ç¡®ç‡ï¼ˆ8x8ï¼‰
        print("\nã€å…¨å±€å¹³å‡å‡†ç¡®ç‡ - 8x8ã€‘")
        if grid_size_key in self.global_averages:
            print(f"  8x8: {self.global_averages[grid_size_key]}")

        # æ¨¡æ€å¹³å‡å‡†ç¡®ç‡ï¼ˆæŒ‰8x8æ’åºåå‰5ä¸ªï¼‰
        print("\nã€æ¨¡æ€å¹³å‡å‡†ç¡®ç‡ - 8x8ï¼ˆå‰5ä¸ªï¼‰ã€‘")

        # è®¡ç®—æ¯ä¸ªæ¨¡æ€åœ¨8x8ä¸‹çš„å¹³å‡å‡†ç¡®ç‡ï¼ˆå¹³å‡æ‰€æœ‰é˜ˆå€¼ï¼‰
        modality_avg_8x8 = []
        for modality, grid_data in self.modality_averages.items():
            if grid_size_key in grid_data:
                acc_dict = grid_data[grid_size_key]
                mean_acc = np.mean(list(acc_dict.values()))
                modality_avg_8x8.append((modality, mean_acc))
    
        # æŒ‰å¹³å‡å‡†ç¡®ç‡æ’åºï¼Œå–å‰5
        top5_modalities = sorted(modality_avg_8x8, key=lambda x: x[1], reverse=True)[:5]
        for modality, acc in top5_modalities:
            print(f"  {modality}: å¹³å‡å‡†ç¡®ç‡ = {acc:.4f}")
            accs = self.modality_averages[modality][grid_size_key]
            for thresh, a in sorted(accs.items()):
                print(f"    é˜ˆå€¼ {thresh}px: {a:.4f}")

        # åœºæ™¯å¹³å‡å‡†ç¡®ç‡ï¼ˆæŒ‰8x8æ’åºåå‰5ä¸ªï¼‰
        print("\nã€åœºæ™¯å¹³å‡å‡†ç¡®ç‡ - 8x8ï¼ˆå‰5ä¸ªï¼‰ã€‘")

        scenario_avg_8x8 = []
        for scenario, grid_data in self.scenario_averages.items():
            if grid_size_key in grid_data:
                acc_dict = grid_data[grid_size_key]
                mean_acc = np.mean(list(acc_dict.values()))
                scenario_avg_8x8.append((scenario, mean_acc))

        # æ’åº + å–å‰5
        top5_scenarios = sorted(scenario_avg_8x8, key=lambda x: x[1], reverse=True)[:5]
        for scenario, acc in top5_scenarios:
            print(f"  {scenario}: å¹³å‡å‡†ç¡®ç‡ = {acc:.4f}")
            accs = self.scenario_averages[scenario][grid_size_key]
            for thresh, a in sorted(accs.items()):
                print(f"    é˜ˆå€¼ {thresh}px: {a:.4f}")

        print("\nè¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ç”Ÿæˆçš„Excelæ–‡ä»¶ã€‚")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€åŒ¹é…ç»“æœå‡†ç¡®ç‡ç»Ÿè®¡è„šæœ¬')
    parser.add_argument('--output_dir', 
                       default="/media/jackew/Extreme SSD/newData/MultiModal_Output",
                       help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: /media/jackew/Extreme SSD/newData/MultiModal_Output)')
    parser.add_argument('--exclude_modalities', 
                       nargs='*', 
                       default=[],
                       help='è¦æ’é™¤çš„æ¨¡æ€åˆ—è¡¨ï¼Œä¾‹å¦‚: --exclude_modalities OpticalFlow FreqDomain')
    parser.add_argument('--excel_output',
                       default="match_results_analysis.xlsx",
                       help='Excelè¾“å‡ºæ–‡ä»¶å (é»˜è®¤: match_results_analysis.xlsx)')
    
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºç›®å½•è·¯å¾„
    output_dir = args.output_dir
    
    if not os.path.exists(output_dir):
        print(f"é”™è¯¯ï¼šè¾“å‡ºç›®å½•ä¸å­˜åœ¨ - {output_dir}")
        return
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("="*80)
    print("å¤šæ¨¡æ€åŒ¹é…ç»“æœåˆ†æé…ç½®")
    print("="*80)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ’é™¤æ¨¡æ€: {args.exclude_modalities if args.exclude_modalities else 'æ— '}")
    print(f"Excelè¾“å‡º: {args.excel_output}")
    print("="*80)
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = MatchResultAnalyzer(output_dir, exclude_modalities=args.exclude_modalities)
    analyzer.run_analysis()
    
    # ä¿å­˜ç»“æœå¹¶æ‰“å°æ‘˜è¦
    analyzer.save_results_to_excel(args.excel_output)
    analyzer.print_summary()

if __name__ == "__main__":
    main()
