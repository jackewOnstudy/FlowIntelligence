#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€åŒ¹é…ç»“æœå¯è§†åŒ–è„šæœ¬ - åŸºäºçœŸå®æ•°æ®
å®ç°æ–¹æ¡ˆ3ï¼šåˆ†å±‚å±•ç¤º - çƒ­åŠ›å›¾ + å…³é”®åˆ‡é¢æŠ˜çº¿å›¾

ç‰¹ç‚¹:
- ç›´æ¥ä»åŸå§‹åŒ¹é…æ–‡ä»¶è®¡ç®—çœŸå®çš„æŒ‰ç™¾åˆ†æ¯”å‡†ç¡®ç‡
- æ‰€æœ‰å›¾è¡¨æ•°æ®éƒ½åŸºäºçœŸå®çš„RANSACè®¡ç®—ç»“æœ
- æ”¯æŒå¤šåœºæ™¯ã€å¤šæ¨¡æ€é…å¯¹çš„æ•°æ®èšåˆå±•ç¤º

ä¾èµ–å®‰è£…:
pip install matplotlib seaborn plotly kaleido pandas numpy opencv-python

ä½¿ç”¨æ–¹æ³•:
1. å…ˆè¿è¡Œ analyze_match_results.py ç”ŸæˆExcelåˆ†æç»“æœ
2. è¿è¡Œæ­¤è„šæœ¬ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨:
   python visualize_match_results.py --data_dir /path/to/original/data --excel_file match_results_analysis.xlsx
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import argparse
from typing import Dict, List, Tuple, Optional
import sys
sys.path.append('.')
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç¾è§‚ä¸»é¢˜
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

class MatchResultVisualizer:
    def __init__(self, excel_file: str, data_dir: str, scene_prefix: str = 'V'):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            excel_file: Excelåˆ†æç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè·å–æ¨¡æ€é…å¯¹ä¿¡æ¯ï¼‰
            data_dir: åŸå§‹æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼Œç”¨äºè®¡ç®—çœŸå®çš„ç™¾åˆ†æ¯”å‡†ç¡®ç‡ï¼‰
            scene_prefix: åœºæ™¯æ–‡ä»¶å¤¹å‰ç¼€ï¼Œä¾‹å¦‚ 'V', 'A', 'K' ç­‰
        """
        self.excel_file = excel_file
        self.data_dir = data_dir
        self.scene_prefix = scene_prefix
        self.data = {}
        self.grid_sizes = ['8x8', '16x16', '32x32', '64x64']
        self.thresholds = [4, 8, 12, 16, 20]  # åƒç´ é˜ˆå€¼
        self.match_percentages = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # åŒ¹é…ç™¾åˆ†æ¯”
        
        # ä¸“ä¸šé…è‰²æ–¹æ¡ˆ
        self.colors = {
            'primary': '#2E86AB',      # æ·±è“
            'secondary': '#A23B72',    # ç´«çº¢
            'accent': '#F18F01',       # æ©™è‰²
            'success': '#C73E1D',      # çº¢è‰²
            'info': '#6A994E',         # ç»¿è‰²
            'background': '#F5F7FA',   # æµ…ç°èƒŒæ™¯
            'text': '#2D3748'          # æ·±ç°æ–‡å­—
        }
        
    def load_data(self):
        """ä»Excelæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            # è¯»å–å„ä¸ªå·¥ä½œè¡¨
            self.data['detailed_accuracy'] = pd.read_excel(self.excel_file, sheet_name='è¯¦ç»†ç»“æœ_å‡†ç¡®ç‡')
            self.data['detailed_matches'] = pd.read_excel(self.excel_file, sheet_name='è¯¦ç»†ç»“æœ_åŒ¹é…æ•°')
            self.data['modality_avg_accuracy'] = pd.read_excel(self.excel_file, sheet_name='æ¨¡æ€å¹³å‡å‡†ç¡®ç‡')
            self.data['scenario_avg_accuracy'] = pd.read_excel(self.excel_file, sheet_name='åœºæ™¯å¹³å‡å‡†ç¡®ç‡')
            self.data['global_avg_accuracy'] = pd.read_excel(self.excel_file, sheet_name='å…¨å±€å¹³å‡å‡†ç¡®ç‡')
            
            print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"åœºæ™¯æ•°é‡: {len(self.data['detailed_accuracy']['åœºæ™¯'].unique())}")
            print(f"æ¨¡æ€é…å¯¹æ•°é‡: {len(self.data['detailed_accuracy']['æ¨¡æ€é…å¯¹'].unique())}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def parse_accuracy_dict(self, acc_str: str) -> Dict[int, float]:
        """è§£æå‡†ç¡®ç‡å­—å…¸å­—ç¬¦ä¸²"""
        if pd.isna(acc_str) or acc_str == "NA":
            return {}
        try:
            # å‡è®¾æ ¼å¼ç±»ä¼¼ "{4: 0.85, 8: 0.92, 12: 0.94, 16: 0.96, 20: 0.97}"
            acc_dict = eval(acc_str) if isinstance(acc_str, str) else acc_str
            return acc_dict if isinstance(acc_dict, dict) else {}
        except:
            return {}
    


    def calculate_real_percentage_data(self, match_file_path: str) -> Dict[int, Dict[int, float]]:
        """
        ä»åŸå§‹åŒ¹é…æ–‡ä»¶è®¡ç®—çœŸå®çš„æŒ‰ç™¾åˆ†æ¯”å‡†ç¡®ç‡æ•°æ®
        
        Args:
            match_file_path: åŒ¹é…ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[percentage, Dict[threshold, accuracy]]
        """
        try:
            # é‡ç”¨ analyze_match_results.py ä¸­çš„å‡½æ•°
            from analyze_match_results import load_match_ids_from_txt, patch_id_to_coords, get_video_resolution_from_match_path
            import cv2
            
            # è¯»å–åŒ¹é…æ•°æ®
            matches = load_match_ids_from_txt(match_file_path)
            if not matches:
                return {}
            
            # è·å–è§†é¢‘åˆ†è¾¨ç‡å’Œpatchå¤§å°
            filename = os.path.basename(match_file_path)
            if 'x' not in filename:
                return {}
            
            patch_size = int(filename.split('x')[0])
            res = get_video_resolution_from_match_path(match_file_path)
            if res is None:
                return {}
            
            width, height = res
            
            # è®¡ç®—åæ ‡
            src_pts = np.array([patch_id_to_coords(m[0], patch_size, width) for m in matches])
            dst_pts = np.array([patch_id_to_coords(m[1], patch_size, width) for m in matches])
            
            result_data = {}
            
            # å¯¹æ¯ä¸ªç™¾åˆ†æ¯”è®¡ç®—å‡†ç¡®ç‡
            for pct in self.match_percentages:
                # å–å‰pct%çš„åŒ¹é…
                num_matches = int(len(matches) * pct / 100)
                if num_matches < 4:  # RANSACæœ€å°‘éœ€è¦4ä¸ªç‚¹
                    continue
                
                subset_src = src_pts[:num_matches]
                subset_dst = dst_pts[:num_matches]
                
                # æ£€æŸ¥åŒ¹é…ç‚¹æ•°é‡æ˜¯å¦è¶³å¤Ÿè¿›è¡ŒRANSAC
                if num_matches < 4:
                    # åŒ¹é…ç‚¹æ•°é‡ä¸è¶³ï¼Œè·³è¿‡RANSACï¼Œç›´æ¥è®¡ç®—å‡†ç¡®ç‡
                    inlier_src = subset_src
                    inlier_dst = subset_dst
                else:
                    # ä½¿ç”¨RANSACè®¡ç®—å†…ç‚¹
                    H, mask = cv2.findHomography(
                        subset_src, subset_dst,
                        method=cv2.RANSAC,
                        ransacReprojThreshold=max(3.0, patch_size * 0.5),
                        maxIters=2000,
                        confidence=0.95
                    )
                    
                    if mask is None:
                        # RANSACå¤±è´¥ï¼Œä½¿ç”¨æ‰€æœ‰åŒ¹é…ç‚¹
                        inlier_src = subset_src
                        inlier_dst = subset_dst
                    else:
                        # æå–å†…ç‚¹
                        inlier_mask = mask.ravel() == 1
                        inlier_src = subset_src[inlier_mask]
                        inlier_dst = subset_dst[inlier_mask]
                        
                        if len(inlier_src) == 0:
                            # æ²¡æœ‰å†…ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰åŒ¹é…ç‚¹
                            inlier_src = subset_src
                            inlier_dst = subset_dst
                
                # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„å‡†ç¡®ç‡
                threshold_acc = {}
                for threshold in self.thresholds:
                    # è®¡ç®—åƒç´ çº§è¯¯å·® (ä½¿ç”¨ max(dx, dy) ä¸ analyze_match_results.py ä¿æŒä¸€è‡´)
                    dx = np.abs(inlier_src[:, 0] - inlier_dst[:, 0])
                    dy = np.abs(inlier_src[:, 1] - inlier_dst[:, 1])
                    errors = np.maximum(dx, dy)
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    inliers = np.sum(errors <= threshold)
                    accuracy = inliers / len(errors) if len(errors) > 0 else 0.0
                    threshold_acc[threshold] = accuracy
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é˜ˆå€¼ä¸‹çš„å‡†ç¡®ç‡éƒ½ä¸º0ï¼Œå¦‚æœæ˜¯åˆ™é‡æ–°è®¡ç®—
                if sum(threshold_acc.values()) == 0:
                    # æ­£ç¡®åŒ¹é…è¿‡å°‘ï¼ŒRANSACç­›é€‰å‡ºæ¥çš„inlier pointå…¶å®æ˜¯é”™è¯¯çš„ï¼Œä½¿ç”¨æ‰€æœ‰åŸå§‹åŒ¹é…ç‚¹
                    inlier_src = subset_src
                    inlier_dst = subset_dst
                    
                    # é‡æ–°è®¡ç®—å‡†ç¡®ç‡
                    threshold_acc = {}
                    for threshold in self.thresholds:
                        dx = np.abs(inlier_src[:, 0] - inlier_dst[:, 0])
                        dy = np.abs(inlier_src[:, 1] - inlier_dst[:, 1])
                        errors = np.maximum(dx, dy)
                        
                        inliers = np.sum(errors <= threshold)
                        accuracy = inliers / len(errors) if len(errors) > 0 else 0.0
                        threshold_acc[threshold] = accuracy
                
                result_data[pct] = threshold_acc
            
            return result_data
            
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è®¡ç®—çœŸå®ç™¾åˆ†æ¯”æ•°æ®: {e}")
            return {}
    
    def create_heatmap_data(self) -> pd.DataFrame:
        """åˆ›å»ºçƒ­åŠ›å›¾æ•°æ® - ä½¿ç”¨çœŸå®æ•°æ®"""
        heatmap_data = []
        
        print("ğŸ” ä»åŸå§‹åŒ¹é…æ–‡ä»¶è®¡ç®—çœŸå®çš„ç™¾åˆ†æ¯”å‡†ç¡®ç‡...")
        
        # è·å–æ‰€æœ‰åœºæ™¯å’Œæ¨¡æ€é…å¯¹
        scenarios = []
        modality_pairs = []
        
        # ä»Excelæ–‡ä»¶ä¸­è·å–åœºæ™¯å’Œæ¨¡æ€é…å¯¹ä¿¡æ¯
        if 'detailed_accuracy' in self.data:
            scenarios = self.data['detailed_accuracy']['åœºæ™¯'].unique().tolist()
            modality_pairs = self.data['detailed_accuracy']['æ¨¡æ€é…å¯¹'].unique().tolist()
        else:
            # å¦‚æœExcelæ•°æ®ä¸å¯ç”¨ï¼Œç›´æ¥æ‰«ææ•°æ®ç›®å½•
            scenarios, modality_pairs = self._scan_data_directory()
        
        # å¯¹æ¯ä¸ªåœºæ™¯ã€æ¨¡æ€é…å¯¹å’Œç½‘æ ¼å¤§å°è®¡ç®—çœŸå®å‡†ç¡®ç‡
        total_combinations = len(scenarios) * len(modality_pairs) * len(self.grid_sizes)
        current_count = 0
        
        for scenario in scenarios:
            for modality_pair in modality_pairs:
                for grid_size in self.grid_sizes:
                    current_count += 1
                    print(f"å¤„ç†è¿›åº¦: {current_count}/{total_combinations} - {scenario}/{modality_pair}/{grid_size}")
                    
                    # æ‰¾åˆ°å¯¹åº”çš„åŒ¹é…æ–‡ä»¶
                    match_file_path = self._find_match_file(scenario, modality_pair, grid_size)
                    
                    if match_file_path and os.path.exists(match_file_path):
                        # è®¡ç®—çœŸå®çš„ç™¾åˆ†æ¯”å‡†ç¡®ç‡æ•°æ®
                        pct_accuracy_data = self.calculate_real_percentage_data(match_file_path)
                        
                        # å°†æ•°æ®æ·»åŠ åˆ°çƒ­åŠ›å›¾æ•°æ®ä¸­
                        for pct in self.match_percentages:
                            if pct in pct_accuracy_data:
                                for threshold in self.thresholds:
                                    if threshold in pct_accuracy_data[pct]:
                                        accuracy = pct_accuracy_data[pct][threshold]
                                        heatmap_data.append({
                                            'Scenario': scenario,
                                            'Modality Pair': modality_pair,
                                            'Grid Size': grid_size,
                                            'Error Threshold (px)': threshold,
                                            'Match Percentage (%)': pct,
                                            'Accuracy': accuracy
                                        })
        
        return pd.DataFrame(heatmap_data)
    
    def _scan_data_directory(self) -> Tuple[List[str], List[str]]:
        """æ‰«ææ•°æ®ç›®å½•è·å–åœºæ™¯å’Œæ¨¡æ€é…å¯¹"""
        scenarios = []
        modality_pairs = set()
        
        for item in os.listdir(self.data_dir):
            item_path = os.path.join(self.data_dir, item)
            if os.path.isdir(item_path) and item.startswith(self.scene_prefix):
                scenarios.append(item)
                
                # æ‰«ææ¨¡æ€é…å¯¹
                for modality_item in os.listdir(item_path):
                    modality_path = os.path.join(item_path, modality_item)
                    if os.path.isdir(modality_path) and '_vs_' in modality_item:
                        modality_pairs.add(modality_item)
        
        scenarios.sort()
        modality_pairs = sorted(list(modality_pairs))
        
        print(f"ğŸ“Š å‘ç° {len(scenarios)} ä¸ªåœºæ™¯: {scenarios[:5]}{'...' if len(scenarios) > 5 else ''}")
        print(f"ğŸ“Š å‘ç° {len(modality_pairs)} ä¸ªæ¨¡æ€é…å¯¹: {modality_pairs[:3]}{'...' if len(modality_pairs) > 3 else ''}")
        
        return scenarios, modality_pairs
    
    def _find_match_file(self, scenario: str, modality_pair: str, grid_size: str) -> Optional[str]:
        """æ‰¾åˆ°å¯¹åº”çš„åŒ¹é…æ–‡ä»¶è·¯å¾„"""
        # æ„å»ºæ–‡ä»¶è·¯å¾„: data_dir/scenario/modality_pair/MatchResult/List/subdir/grid_size.txt
        base_path = os.path.join(self.data_dir, scenario, modality_pair, 'MatchResult', 'List')
        
        if not os.path.exists(base_path):
            return None
        
        try:
            # æ‰¾åˆ°Listä¸‹çš„å­æ–‡ä»¶å¤¹
            subdirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
            if not subdirs:
                return None
            
            # å–ç¬¬ä¸€ä¸ªå­æ–‡ä»¶å¤¹
            subdir = subdirs[0]
            match_file = os.path.join(base_path, subdir, f"{grid_size}.txt")
            
            return match_file if os.path.exists(match_file) else None
            
        except Exception as e:
            print(f"âš ï¸ æ‰«æè·¯å¾„å‡ºé”™ {base_path}: {e}")
            return None
    
    def plot_main_heatmap(self, data: pd.DataFrame, output_dir: str):
        """ç»˜åˆ¶ä¸»çƒ­åŠ›å›¾"""
        # ä¸ºæ¯ä¸ªç½‘æ ¼å¤§å°åˆ›å»ºçƒ­åŠ›å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Multimodal Matching Accuracy Heatmap\n(Error Threshold vs Match Percentage)', fontsize=16, fontweight='bold')
        
        axes = axes.flatten()
        
        for idx, grid_size in enumerate(self.grid_sizes):
            grid_data = data[data['Grid Size'] == grid_size]
            
            if grid_data.empty:
                continue
            
            # å¯¹å¤šä¸ªåœºæ™¯å’Œæ¨¡æ€é…å¯¹çš„æ•°æ®è¿›è¡Œå¹³å‡
            aggregated_data = grid_data.groupby(['Error Threshold (px)', 'Match Percentage (%)'])['Accuracy'].mean().reset_index()
                
            # åˆ›å»ºé€è§†è¡¨
            pivot_data = aggregated_data.pivot(index='Error Threshold (px)', 
                                             columns='Match Percentage (%)', 
                                             values='Accuracy')
            
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            im = sns.heatmap(pivot_data, 
                           annot=True, 
                           fmt='.3f',
                           cmap='RdYlBu_r',
                           ax=axes[idx],
                           cbar_kws={'label': 'Accuracy'},
                           annot_kws={'size': 8})
            
            axes[idx].set_title(f'Grid Size: {grid_size}', fontweight='bold', fontsize=12)
            axes[idx].set_xlabel('Match Percentage (%)', fontweight='bold')
            axes[idx].set_ylabel('Error Threshold (pixels)', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'heatmap_accuracy_overview.png'), 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print("âœ… ä¸»çƒ­åŠ›å›¾å·²ç”Ÿæˆ: heatmap_accuracy_overview.png")
    
    def plot_threshold_curves(self, data: pd.DataFrame, output_dir: str):
        """ç»˜åˆ¶å…³é”®é˜ˆå€¼æ›²çº¿å›¾"""
        key_thresholds = [4, 8, 16]  # é€‰æ‹©ä»£è¡¨æ€§é˜ˆå€¼
        key_grid = '8x8'  # é‡ç‚¹å…³æ³¨8x8ç½‘æ ¼
        
        plt.figure(figsize=(12, 8))
        
        grid_data = data[data['Grid Size'] == key_grid]
        
        # å¯¹æ•°æ®è¿›è¡Œèšåˆ
        aggregated_data = grid_data.groupby(['Error Threshold (px)', 'Match Percentage (%)'])['Accuracy'].mean().reset_index()
        
        for threshold in key_thresholds:
            thresh_data = aggregated_data[aggregated_data['Error Threshold (px)'] == threshold]
            if not thresh_data.empty:
                plt.plot(thresh_data['Match Percentage (%)'], 
                        thresh_data['Accuracy'],
                        marker='o', 
                        linewidth=2.5,
                        markersize=6,
                        label=f'{threshold}px Threshold',
                        alpha=0.8)
        
        plt.title(f'Accuracy vs Match Percentage (Grid: {key_grid})', fontweight='bold', fontsize=14)
        plt.xlabel('Match Percentage (%)', fontweight='bold', fontsize=12)
        plt.ylabel('Accuracy', fontweight='bold', fontsize=12)
        plt.legend(frameon=True, fancybox=True, shadow=True)
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)
        
        # æ·»åŠ æœ€ä½³å·¥ä½œç‚¹æ ‡æ³¨
        for threshold in key_thresholds:
            thresh_data = aggregated_data[aggregated_data['Error Threshold (px)'] == threshold]
            if not thresh_data.empty:
                # æ‰¾åˆ°å‡†ç¡®ç‡æœ€é«˜çš„ç‚¹
                best_idx = thresh_data['Accuracy'].idxmax()
                best_row = thresh_data.loc[best_idx]
                plt.annotate(f'Best Point\n({best_row["Match Percentage (%)"]}%, {best_row["Accuracy"]:.3f})',
                           xy=(best_row['Match Percentage (%)'], best_row['Accuracy']),
                           xytext=(10, 10), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                           arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'threshold_curves.png'), 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print("âœ… é˜ˆå€¼æ›²çº¿å›¾å·²ç”Ÿæˆ: threshold_curves.png")
    
    def plot_percentage_curves(self, data: pd.DataFrame, output_dir: str):
        """ç»˜åˆ¶å…³é”®ç™¾åˆ†æ¯”æ›²çº¿å›¾"""
        key_percentages = [25, 50, 75]  # é€‰æ‹©ä»£è¡¨æ€§ç™¾åˆ†æ¯”
        key_grid = '8x8'
        
        plt.figure(figsize=(12, 8))
        
        grid_data = data[data['Grid Size'] == key_grid]
        
        # å¯¹æ•°æ®è¿›è¡Œèšåˆ
        aggregated_data = grid_data.groupby(['Error Threshold (px)', 'Match Percentage (%)'])['Accuracy'].mean().reset_index()
        
        for pct in key_percentages:
            pct_data = aggregated_data[aggregated_data['Match Percentage (%)'] == pct]
            if not pct_data.empty:
                plt.plot(pct_data['Error Threshold (px)'], 
                        pct_data['Accuracy'],
                        marker='s', 
                        linewidth=2.5,
                        markersize=6,
                        label=f'Top {pct}% Matches',
                        alpha=0.8)
        
        plt.title(f'Accuracy vs Error Threshold (Grid: {key_grid})', fontweight='bold', fontsize=14)
        plt.xlabel('Error Threshold (pixels)', fontweight='bold', fontsize=12)
        plt.ylabel('Accuracy', fontweight='bold', fontsize=12)
        plt.legend(frameon=True, fancybox=True, shadow=True)
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'percentage_curves.png'), 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print("âœ… ç™¾åˆ†æ¯”æ›²çº¿å›¾å·²ç”Ÿæˆ: percentage_curves.png")
    
    def plot_interactive_3d(self, data: pd.DataFrame, output_dir: str):
        """åˆ›å»ºäº¤äº’å¼3Då›¾è¡¨"""
        key_grid = '8x8'
        grid_data = data[data['Grid Size'] == key_grid]
        
        if grid_data.empty:
            return
        
        # å¯¹æ•°æ®è¿›è¡Œèšåˆ
        aggregated_data = grid_data.groupby(['Error Threshold (px)', 'Match Percentage (%)'])['Accuracy'].mean().reset_index()
        
        # åˆ›å»º3Dæ•£ç‚¹å›¾
        fig = go.Figure(data=[go.Scatter3d(
            x=aggregated_data['Match Percentage (%)'],
            y=aggregated_data['Error Threshold (px)'],
            z=aggregated_data['Accuracy'],
            mode='markers',
            marker=dict(
                size=8,
                color=aggregated_data['Accuracy'],
                colorscale='Viridis',
                opacity=0.8,
                colorbar=dict(title="Accuracy")
            ),
            text=[f'Percentage: {p}%<br>Threshold: {t}px<br>Accuracy: {a:.3f}' 
                  for p, t, a in zip(aggregated_data['Match Percentage (%)'], 
                                    aggregated_data['Error Threshold (px)'], 
                                    aggregated_data['Accuracy'])],
            hovertemplate='%{text}<extra></extra>'
        )])
        
        fig.update_layout(
            title=f'3D Match Result Analysis (Grid: {key_grid})',
            scene=dict(
                xaxis_title='Match Percentage (%)',
                yaxis_title='Error Threshold (pixels)',
                zaxis_title='Accuracy',
                bgcolor='white'
            ),
            width=800,
            height=600
        )
        
        # ä¿å­˜ä¸ºHTML
        fig.write_html(os.path.join(output_dir, 'interactive_3d_plot.html'))
        print("âœ… äº¤äº’å¼3Då›¾å·²ç”Ÿæˆ: interactive_3d_plot.html")
    
    def plot_modality_comparison(self, output_dir: str):
        """ç»˜åˆ¶æ¨¡æ€å¯¹æ¯”å›¾"""
        plt.figure(figsize=(14, 10))
        
        # è§£ææ¨¡æ€å¹³å‡å‡†ç¡®ç‡æ•°æ®
        modality_data = []
        for _, row in self.data['modality_avg_accuracy'].iterrows():
            modality = row['æ¨¡æ€é…å¯¹']
            for grid_size in self.grid_sizes:
                acc_col = f'å¹³å‡å‡†ç¡®ç‡_{grid_size}'
                if acc_col in row and not pd.isna(row[acc_col]):
                    acc_dict = self.parse_accuracy_dict(row[acc_col])
                    if acc_dict:
                        # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
                        avg_acc = np.mean(list(acc_dict.values()))
                        modality_data.append({
                            'æ¨¡æ€é…å¯¹': modality,
                            'ç½‘æ ¼å¤§å°': grid_size,
                            'å¹³å‡å‡†ç¡®ç‡': avg_acc
                        })
        
        if modality_data:
            df_modality = pd.DataFrame(modality_data)
            
            # é€‰æ‹©å‰10ä¸ªæ¨¡æ€è¿›è¡Œå¯è§†åŒ–
            top_modalities = df_modality.groupby('æ¨¡æ€é…å¯¹')['å¹³å‡å‡†ç¡®ç‡'].mean().nlargest(10).index
            df_plot = df_modality[df_modality['æ¨¡æ€é…å¯¹'].isin(top_modalities)]
            
            # åˆ›å»ºåˆ†ç»„æŸ±çŠ¶å›¾
            pivot_modality = df_plot.pivot(index='æ¨¡æ€é…å¯¹', columns='ç½‘æ ¼å¤§å°', values='å¹³å‡å‡†ç¡®ç‡')
            
            ax = pivot_modality.plot(kind='bar', 
                                   figsize=(14, 8), 
                                   color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'],
                                   alpha=0.8)
            
            plt.title('Accuracy Comparison of Different Modality Pairs (Top 10)', fontweight='bold', fontsize=14)
            plt.xlabel('Modality Pairs', fontweight='bold', fontsize=12)
            plt.ylabel('Average Accuracy', fontweight='bold', fontsize=12)
            plt.legend(title='Grid Size', frameon=True, fancybox=True, shadow=True)
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3, axis='y')
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'modality_comparison.png'), 
                       dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print("âœ… æ¨¡æ€å¯¹æ¯”å›¾å·²ç”Ÿæˆ: modality_comparison.png")
    
    def generate_summary_dashboard(self, data: pd.DataFrame, output_dir: str):
        """ç”Ÿæˆç»¼åˆä»ªè¡¨æ¿"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Global Accuracy Heatmap', '8x8 Grid Threshold Curves', '8x8 Grid Percentage Impact', 'Grid Size Comparison'),
            specs=[[{"type": "heatmap"}, {"type": "scatter"}],
                   [{"type": "scatter"}, {"type": "bar"}]]
        )
        
        # 1. çƒ­åŠ›å›¾ (ä½¿ç”¨8x8æ•°æ®)
        key_grid = '8x8'
        grid_data = data[data['Grid Size'] == key_grid]
        if not grid_data.empty:
            # å¯¹æ•°æ®è¿›è¡Œèšåˆ
            aggregated_data = grid_data.groupby(['Error Threshold (px)', 'Match Percentage (%)'])['Accuracy'].mean().reset_index()
            pivot_data = aggregated_data.pivot(index='Error Threshold (px)', 
                                             columns='Match Percentage (%)', 
                                             values='Accuracy')
            
            fig.add_trace(
                go.Heatmap(
                    z=pivot_data.values,
                    x=pivot_data.columns,
                    y=pivot_data.index,
                    colorscale='RdYlBu_r',
                    showscale=False
                ),
                row=1, col=1
            )
        
        # 2. é˜ˆå€¼æ›²çº¿
        for threshold in [4, 8, 16]:
            thresh_data = aggregated_data[aggregated_data['Error Threshold (px)'] == threshold]
            if not thresh_data.empty:
                fig.add_trace(
                    go.Scatter(
                        x=thresh_data['Match Percentage (%)'],
                        y=thresh_data['Accuracy'],
                        mode='lines+markers',
                        name=f'{threshold}px',
                        showlegend=False
                    ),
                    row=1, col=2
                )
        
        # 3. ç™¾åˆ†æ¯”å½±å“
        for pct in [25, 50, 75]:
            pct_data = aggregated_data[aggregated_data['Match Percentage (%)'] == pct]
            if not pct_data.empty:
                fig.add_trace(
                    go.Scatter(
                        x=pct_data['Error Threshold (px)'],
                        y=pct_data['Accuracy'],
                        mode='lines+markers',
                        name=f'{pct}%',
                        showlegend=False
                    ),
                    row=2, col=1
                )
        
        # 4. ç½‘æ ¼å¤§å°å¯¹æ¯”
        grid_comparison = []
        for grid_size in self.grid_sizes:
            grid_subset = data[data['Grid Size'] == grid_size]
            if not grid_subset.empty:
                avg_acc = grid_subset['Accuracy'].mean()
                grid_comparison.append((grid_size, avg_acc))
        
        if grid_comparison:
            grid_names, grid_accs = zip(*grid_comparison)
            fig.add_trace(
                go.Bar(
                    x=list(grid_names),
                    y=list(grid_accs),
                    showlegend=False
                ),
                row=2, col=2
            )
        
        fig.update_layout(
            height=800,
            title_text="Multimodal Matching Results - Comprehensive Analysis Dashboard",
            title_x=0.5
        )
        
        fig.write_html(os.path.join(output_dir, 'dashboard.html'))
        print("âœ… ç»¼åˆä»ªè¡¨æ¿å·²ç”Ÿæˆ: dashboard.html")
    
    def run_visualization(self, output_dir: str = "visualization_output"):
        """è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–æµç¨‹"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        print("ğŸ¨ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
        heatmap_data = self.create_heatmap_data()
        
        # 3. ç”Ÿæˆå„ç§å›¾è¡¨
        self.plot_main_heatmap(heatmap_data, output_dir)
        self.plot_threshold_curves(heatmap_data, output_dir)
        self.plot_percentage_curves(heatmap_data, output_dir)
        self.plot_interactive_3d(heatmap_data, output_dir)
        self.plot_modality_comparison(output_dir)
        self.generate_summary_dashboard(heatmap_data, output_dir)
        
        print(f"\nğŸ‰ æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print("ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - heatmap_accuracy_overview.png: ä¸»çƒ­åŠ›å›¾")
        print("  - threshold_curves.png: é˜ˆå€¼æ›²çº¿å›¾")
        print("  - percentage_curves.png: ç™¾åˆ†æ¯”æ›²çº¿å›¾")
        print("  - modality_comparison.png: æ¨¡æ€å¯¹æ¯”å›¾")
        print("  - interactive_3d_plot.html: äº¤äº’å¼3Då›¾")
        print("  - dashboard.html: ç»¼åˆä»ªè¡¨æ¿")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€åŒ¹é…ç»“æœå¯è§†åŒ–è„šæœ¬ - ä½¿ç”¨çœŸå®æ•°æ®')
    parser.add_argument('--excel_file', 
                       default="match_results_analysis.xlsx",
                       help='Excelåˆ†æç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè·å–æ¨¡æ€é…å¯¹ä¿¡æ¯ï¼‰')
    parser.add_argument('--output_dir',
                       default="./results/visualization_output",
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--data_dir',
                       required=True,
                       help='åŸå§‹æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼Œç”¨äºè®¡ç®—çœŸå®çš„ç™¾åˆ†æ¯”å‡†ç¡®ç‡ï¼‰')
    parser.add_argument('--scene_prefix',
                       default='V',
                       help='åœºæ™¯æ–‡ä»¶å¤¹å‰ç¼€ï¼Œä¾‹å¦‚ V, A, K ç­‰ (é»˜è®¤: V)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.excel_file):
        print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {args.excel_file}")
        print("è¯·å…ˆè¿è¡Œ analyze_match_results.py ç”Ÿæˆåˆ†æç»“æœ")
        return
    
    if not os.path.exists(args.data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        return
    
    print("ğŸ¯ ä½¿ç”¨çœŸå®æ•°æ®è®¡ç®—ç™¾åˆ†æ¯”å‡†ç¡®ç‡...")
    print(f"ğŸ“ Excelæ–‡ä»¶: {args.excel_file}")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ¯ åœºæ™¯å‰ç¼€: {args.scene_prefix}")
    
    # åˆ›å»ºå¯è§†åŒ–å™¨å¹¶è¿è¡Œ
    visualizer = MatchResultVisualizer(args.excel_file, args.data_dir, args.scene_prefix)
    visualizer.run_visualization(args.output_dir)

if __name__ == "__main__":
    main()
